{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "\t<font size=5>\n",
    "\t\t<div align=center>\n",
    "\t\t\t\t\t\t<font color=#FF7500>\n",
    "Sharif University of Technology - CE Department \n",
    "            </font>\n",
    "\t\t\t<p></p>\n",
    "\t\t\t<font color=blue>\n",
    "Artificial Intelligence\n",
    "            </font>\n",
    "\t\t\t<br />\n",
    "\t\t\t<br />\n",
    "Spring 2021\n",
    "\t\t</div>\n",
    "\t\t<hr/>\n",
    "\t\t<font color=red size=6>\n",
    "\t\t\t<br />\n",
    "\t\t\t<div align=center>\n",
    "Convolutional Neural Networks\n",
    "            </div>\n",
    "\t\t</font>\n",
    "\t\t<br />\n",
    "\t\t<div align=center>\n",
    " Parsa Hosseini, Alireza Dehghanpour, Mahdi Salmani\n",
    "        </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Convolutional Neural Networks**\n",
    "Table of Contents:\n",
    "* Impact\n",
    "* Learning Visual Features\n",
    "* CovNet Layers:\n",
    "  * Convolutional Layer\n",
    "  * Pooling Layer\n",
    "  * Fully-Connected Layer\n",
    "* Conclusion\n",
    "* References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact\n",
    "Convolutional Neural Networks, or CNNs, were designed to map image data to an output variable.\n",
    "\n",
    "They have proven so effective that they are the go-to method for any type of prediction problem involving image data as an input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](impact.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](impact2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Visual Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An image is just a matrix of numbers [0,255]. i.e., 1080x1080x3 for an RGB image. <br>\n",
    "Output variable produces probabilty of belonging to a particular class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](Lincoln.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic solution is to using fully connected neural networks:\n",
    "![image.png](FC.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular Neural Nets don’t scale well to full images. In MNIST, images are only of size 28x28x1 (28 wide, 28 high, gray scale), so a single fully-connected neuron in a first hidden layer of a regular Neural Network would have 28x28x1 = 784 weights. This amount still seems manageable, but clearly this fully-connected structure does not scale to larger images. For example, an image of more respectable size, e.g. 200x200x3, would lead to neurons that have 200*200*3 = 120,000 weights. Moreover, we would almost certainly want to have several such neurons, so the parameters would add up quickly! Clearly, this full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting\n",
    "\n",
    "![hot.gif](hot.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CovNet Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four types of layers in a Convolutional Neural Network:\n",
    "1.   Convolutional Layers\n",
    "2.   Pooling Layers\n",
    "3.   Fully-Connected Layers\n",
    "\n",
    "![image.png](overview.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layers are comprised of filters and feature maps.\n",
    "\n",
    "**Filters** <br>\n",
    "The input image is multiplied by a filter to get the convolved layer. These filters differ in shapes and values to get different features like edges, curves, lines. Filter also called as kernel or feature detector. <br>\n",
    "The filters are the “neurons” of the layer. The have input weights and output a value. The input size is a fixed square called a patch or a receptive field.\n",
    "<br>\n",
    "If the convolutional layer is an input layer, then the input patch will be pixel values. If the deeper in the network architecture, then the convolutional layer will take input from a feature map from the previous layer.\n",
    "![filters.png](filters.png)\n",
    "\n",
    "**Feature Maps** <br>\n",
    "The feature map is the output of one filter applied to the previous layer.\n",
    "\n",
    "A given filter is drawn across the entire previous layer, moved one pixel at a time. Each position results in an activation of the neuron and the output is collected in the feature map.\n",
    "![featureMap.gif](featureMap.gif)\n",
    "In CNN terminology, the 3×3 matrix is called a ‘filter‘ or ‘kernel’ or ‘feature detector’ and the matrix formed by sliding the filter over the image and computing the element-wise multiply is called the ‘Feature Map‘. It is important to note that filters acts as feature detectors from the original input image.\n",
    "![filters.png](featureMap.png)\n",
    "\n",
    "**Zero Padding** <br>\n",
    "The distance that filter is moved across the the input from the previous layer each activation is referred to as the stride.\n",
    "\n",
    "If the size of the previous layer is not cleanly divisible by the size of the filters receptive field and the size of the stride then it is possible for the receptive field to attempt to read off the edge of the input feature map. In this case, techniques like zero padding can be used to invent mock inputs for the receptive field to read.\n",
    "![padding.gif](padding.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After each conv layer, it is convention to apply a nonlinear layer (or activation layer) immediately afterward.The purpose of this layer is to introduce nonlinearity to a system that basically has just been computing linear operations during the conv layers (just element wise multiplications and summations).In the past, nonlinear functions like tanh and sigmoid were used, but researchers found out that ReLU layers work far better because the network is able to train a lot faster (because of the computational efficiency) without making a significant difference to the accuracy. It also helps to alleviate the vanishing gradient problem, which is the issue where the lower layers of the network train very slowly because the gradient decreases exponentially through the layers. The ReLU layer applies the function f(x) = max(0, x) to all of the values in the input volume. In basic terms, this layer just changes all the negative activations to 0. This layer increases the nonlinear properties of the model and the overall network without affecting the receptive fields of the conv layer.\n",
    "![ReLU.png](ReLU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pooling layer (POOL) is a downsampling operation, typically applied after a convolution layer, which does some spatial invariance. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation(called max pooling).\n",
    "![pooling.gif](pooling.gif)\n",
    "In addition to max pooling, the pooling units can also perform other functions, such as average pooling or even L2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to the max pooling operation, which has been shown to work better in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-Connected Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected layers are the normal flat feed-forward neural network layer.\n",
    "\n",
    "These layers may have a non-linear activation function or a softmax activation in order to output probabilities of class predictions.\n",
    "\n",
    "Fully connected layers are used at the end of the network after feature extraction and consolidation has been performed by the convolutional and pooling layers. They are used to create final non-linear combinations of features and for making predictions by the network.\n",
    "![fullyConnected.png](fullyConnected.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding multiple convolutional layers and pooling layers, the image will be processed for feature extraction. And there will be fully connected layers heading to the layer for softmax (for a multi-class case) or sigmoid (for a binary case) function. I didn’t mention the ReLu activation step, but there’s no difference with the activation step in ANN.\n",
    "As the layers go deeper and deeper, the features that the model deals with become more complex. For example, at the early stage of ConvNet, it looks up for oriented line patterns and then finds some simple figures. At the deep stage, it can catch the specific forms of objects and finally able to detect the object of an input image.\n",
    "![conclusion.gif](conclusion.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1.  https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks* <br>\n",
    "*2.  https://mit6874.github.io/assets/sp2021/slides/l03.pdf* <br>\n",
    "*3.  http://introtodeeplearning.com/slides/6S191_MIT_DeepLearning_L3.pdf* <br>\n",
    "*4.  https://towardsdatascience.com/convolution-neural-network-e9b864ac1e6c*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
