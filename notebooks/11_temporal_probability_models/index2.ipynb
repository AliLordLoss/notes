{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Chain Rule and HMMs</strong></p>\n",
    "<p>Look at the bellow model.</p>\n",
    "<p><img src=\"https://i.ibb.co/Fmm8KV4/image-2021-05-13-171737.png\" alt=\"\" width=\"150\" height=\"73\" /></p>\n",
    "<p>From the chain rule, <em>every</em> joint distribution over can be written as:</p>\n",
    "<p><img src=\"https://i.ibb.co/WKws4RT/image-2021-05-13-171904.png\" alt=\"\" width=\"401\" height=\"31\" /></p>\n",
    "<p>And because we have bellow terms:</p>\n",
    "<p><img src=\"https://i.ibb.co/WsD3TpP/image-2021-05-13-172512.png\" alt=\"\" width=\"414\" height=\"11\" /></p>\n",
    "<p>After simplifications we have:</p>\n",
    "<p><img src=\"https://i.ibb.co/LtxSLby/image-2021-05-13-172748.png\" alt=\"\" width=\"425\" height=\"12\" /></p>\n",
    "<p>We can see some of real HMM examples:</p>\n",
    "<ul>\n",
    "<li>Speech recognition HMMs:\n",
    "<ul style=\"list-style-type: square;\">\n",
    "<li>Observations are acoustic signals (continuous valued)</li>\n",
    "<li>States are specific positions in specific words (so, tens of thousands)</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>Machine translation HMMs:\n",
    "<ul>\n",
    "<li>Observations are words (tens of thousands)</li>\n",
    "<li>States are translation options</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>Robot tracking:\n",
    "<ul>\n",
    "<li>Observations are range readings (continuous)</li>\n",
    "<li>States are positions on a map (continuous)</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "<p>&nbsp;</p>\n",
    "<p><strong>Filtering/Monitoring</strong></p>\n",
    "<p>First of all we define B<sub>t</sub>(x) = P(X<sub>t</sub> | E<sub>1</sub>, &hellip;, E<sub>t</sub>) as the belief state and it shows our prediction from next hidden variable according to our observations from start to now. we start from first belief state B<sub>0</sub>(X) in an initial setting (usually uniform) and as time passes or we get observations, we update the value of B<sub>t</sub>(X). In other words we have a vector with lentgh of number of hidden variables and each cell of this vector has our prediction of it's real value. we name this task of tracking the distribution B<sub>t</sub>(X) (actually B(X)) over time \"Flitering\" or \"Monitoring\".</p>\n",
    "<p>&nbsp;</p>\n",
    "<p><strong>Example: Robot Localization</strong></p>\n",
    "<p>Robot localization is the process of determining where a mobile robot is located with respect to its environment. Localization is one of the most fundamental competencies required by an autonomous robot as the knowledge of the robot's own location is an essential precursor to making decisions about future actions. In a typical robot localization scenario, a map of the environment is available and the robot is equipped with sensors that observe the environment as well as monitor its own motion. in this example sensor model can read in which directions there is a wall, never more than 1 mistake and motion model may not execute action with small probability. as mentioned earlier B<sub>0</sub>(X) assigned uniform.</p>\n",
    "<p><img src=\"https://i.ibb.co/tJhcWhd/image-2021-05-13-181644.png\" alt=\"\" width=\"188\" height=\"145\" /></p>\n",
    "<p>Bellow tape shows the colour of each probability for all cells. for example in t=0 each cell has equal probability.</p>\n",
    "<p><img src=\"https://i.ibb.co/n3k41fH/image-2021-05-13-183502.png\" alt=\"\" width=\"188\" height=\"157\" /></p>\n",
    "<p>Cells those are compatible with our evidence from sensors has most probability to be the real place of robot. lighter grey cells are possible to get the reading, but less likely b/c required 1 mistake. white cells need more mistakes so theirs probability is near to zero.</p>\n",
    "<p>After skipping some states we have:</p>\n",
    "<p><img src=\"https://i.ibb.co/r0gPnrP/image-2021-05-13-182944.png\" alt=\"\" width=\"180\" height=\"165\" /></p>\n",
    "<p>And then:</p>\n",
    "<p><img src=\"https://i.ibb.co/xfjNNPZ/image-2021-05-13-183107.png\" alt=\"\" width=\"183\" height=\"169\" /></p>\n",
    "<p>In this state the answer is approximately certain.</p>\n",
    "<p>&nbsp;</p>\n",
    "<p><strong>Passage of Time</strong></p>\n",
    "<p>If in the current state we have the belief B(X<sub>t</sub>)=P(X<sub>t</sub>|e<sub>1:t</sub>). then after one time step passes for P(X<sub>t+1</sub>|e<sub>1:t</sub>) we have:</p>\n",
    "<p><img src=\"https://i.ibb.co/82Ddcrx/image-2021-05-13-184443.png\" width=\"224\" height=\"127\" /></p>\n",
    "<p>We know P(X<sub>t+1</sub>|e<sub>1:t</sub>) isn't what we defined as B<sub>t+1</sub>(X) so we name it B'(X<sub>t+1</sub>) and we have:</p>\n",
    "<p><img src=\"https://i.ibb.co/7nDpTj8/image-2021-05-13-184849.png\" alt=\"\" width=\"159\" height=\"55\" /></p>\n",
    "<p>We name the first part P(X'|x<sub>t</sub>) as transition and say beliefs get &ldquo;pushed&rdquo; through the transitions.</p>\n",
    "<p>&nbsp;</p>\n",
    "<p><strong>Example: Passage of Time</strong></p>\n",
    "<p>In this model as time passes, uncertainty about the answer accumulates and increases.</p>\n",
    "<p><img src=\"https://i.ibb.co/c8LdYxD/image-2021-05-13-185716.png\" alt=\"\" width=\"600\" height=\"120\" /></p>\n",
    "<p>&nbsp;</p>\n",
    "<p><strong>Observation</strong></p>\n",
    "<p>Now we want to affect the observation in our prediction about next value of hidden variable.</p>\n",
    "<p><img src=\"https://i.ibb.co/tPQR0ty/Picture3.jpg\" alt=\"\" width=\"222\" height=\"40\" /></p>\n",
    "<p>And now in each state after observation we have:</p>\n",
    "<p><img src=\"https://i.ibb.co/VCwxHrS/Picture3.jpg\" alt=\"\" width=\"288\" height=\"93\" /></p>\n",
    "<p>We named the second part P(X<sub>t+1</sub>|e<sub>1:t</sub>) as belief and say beliefs get &ldquo;reweighted&rdquo; by likelihood of evidence.</p>\n",
    "<p>** Unlike passage of time, we have to renormalize.</p>\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<p><strong>Example: Observation</strong></p>\n",
    "<p>In this model as we get observations, beliefs get reweighted and uncertainty about the answer decreases.</p>\n",
    "<p><img src=\"https://i.ibb.co/6nd0B29/image-2021-05-13-192007.png\" alt=\"\" width=\"600\" height=\"125\" /></p>\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<p><strong>Example: Weather HMM</strong></p>\n",
    "<p>In this example we want to predict the weather by looking our friend, is he come with umbrella or not. First day we use a uniform distribution but after first day, in each day we compute B' and then after observation of umbrella compute B for that day and do this for each day to decreasing the uncertainty.</p>\n",
    "<p><img src=\"https://i.ibb.co/0V2XBGZ/Picture3.jpg\" alt=\"\" width=\"520\" height=\"186\" /></p>\n",
    "<p>&nbsp;</p>\n",
    "<p><strong>The Forward Algorithm</strong></p>\n",
    "<p>We are given evidence at each time and want to know:</p>\n",
    "<p><img src=\"https://i.ibb.co/F406RmD/image-2021-05-13-193937.png\" alt=\"\" width=\"188\" height=\"21\" /></p>\n",
    "<p>We can derive the following updates:</p>\n",
    "<p><img src=\"https://i.ibb.co/x1PZ6vV/Picture3.jpg\" alt=\"\" width=\"310\" height=\"148\" /></p>\n",
    "<p>We can normalize as we go if we want to have P(x|e) at each time step, or just once at the end. But which is better?</p>\n",
    "<p>&nbsp;</p>\n",
    "<p><strong>Online Belief Updates</strong></p>\n",
    "<p>Every time step, we start with current P(X | evidence)</p>\n",
    "<p>We update for time:</p>\n",
    "<p><img src=\"https://i.ibb.co/WFCXKL0/image-2021-05-13-194923.png\" alt=\"\" width=\"343\" height=\"32\" /></p>\n",
    "<p>We update for evidence:</p>\n",
    "<p><img src=\"https://i.ibb.co/0KYTpLz/image-2021-05-13-195017.png\" alt=\"\" width=\"342\" height=\"21\" /></p>\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "<p><strong>Particle Filtering</strong></p>\n",
    "<p>In some problems |X| is too big for exact computing or even for storing B(X), so it's almost impossible to use previous algorithms. For example when X is continous. In this situations we must use approximate inference.</p>\n",
    "<p>In this algorithm we track just samples of X not all values and name this samples particles. Time per step is linear in the number of samples but number needed may be large enough and in memory should store list of particles not states.</p>\n",
    "<p><img src=\"https://i.ibb.co/2gXcttw/Picture3.jpg\" alt=\"\" width=\"491\" height=\"179\" /></p>\n",
    "<p>Now represent P(X) by a list of N particles and P(x) approximate by number of particles with value of x. We know&nbsp;generally N&lt;&lt;|X| so many x may have p(x)=0 and this isn't good event. For solving this issue we must use more particles to achieve more accuracy.(For now assume all particles has the same weight)</p>\n",
    "<p>&nbsp;</p>\n",
    "<p><strong>Elapse Time</strong></p>\n",
    "<p>Each particle moved by transition model to it's next position.</p>\n",
    "<p><img src=\"https://i.ibb.co/0fXJqqt/image-2021-05-13-213911.png\" alt=\"\" width=\"230\" height=\"26\" /></p>\n",
    "<p>Just like the prior sampling, each sample frequency reflect the transtition probabilities.&nbsp;</p>\n",
    "<p>As mentioned earlier for closing to exact values, must use enough samples.</p>\n",
    "<p><img src=\"https://i.ibb.co/mGSKHnC/Picture3.jpg\" alt=\"\" width=\"490\" height=\"175\" /></p>\n",
    "<p>&nbsp;</p>\n",
    "<p><strong>Observe</strong></p>\n",
    "<p>Just like the likelihood weighting, each sample's&nbsp; probabilities computed based on the evidence.(As before, the probabilities don&rsquo;t sum to one, since all have been down weighted and need to normalizing)</p>\n",
    "<p><img src=\"https://i.ibb.co/t4Nvcnj/Picture3.jpg\" alt=\"\" width=\"398\" height=\"206\" /></p>\n",
    "<p>&nbsp;</p>\n",
    "<p><strong>Resample</strong></p>\n",
    "<p>We use resampling (N times) intead of tracking weighted samples in this way&nbsp;choose from our weighted sample distribution (i.e. draw with replacement). This method is&nbsp;equivalent to renormalizing the distribution.</p>\n",
    "<p>And now the update is complete for this time step, continue with the next one.</p>\n",
    "<p><img src=\"https://i.ibb.co/ZNdKB8k/Picture3.jpg\" alt=\"\" width=\"407\" height=\"135\" /></p>\n",
    "<p>&nbsp;</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
